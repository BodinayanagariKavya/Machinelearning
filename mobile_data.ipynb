{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn.functional import mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"C:/Users/hp/Desktop/pandas/Mobile dataset.csv\")\n",
    "dataset = dataset.drop([\"model\",\"base_color\",\"processor\"], axis=1)\n",
    "dataset[\"battery_capacity\"] = dataset[\"battery_capacity\"]/1000\n",
    "dataset[\"num_of_ratings\"] = dataset[\"num_of_ratings\"]/10000\n",
    "dataset[\"sales_price\"] = dataset[\"sales_price\"]/1000\n",
    "dataset[\"ROM\"] = dataset[\"ROM\"]/100\n",
    "label = dataset[\"sales\"]\n",
    "feature = dataset.drop(\"sales\",axis=1)\n",
    "feature = pd.get_dummies(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([430, 20]) torch.Size([430, 1])\n",
      "tensor([[-0.6617, -1.5233, -4.5237,  ..., -0.2930, -0.0969, 10.3199],\n",
      "        [-0.6617, -0.6059, -2.6272,  ...,  3.4128, -0.0969, -0.0969],\n",
      "        [-0.6617, -1.5233, -4.5237,  ..., -0.2930, -0.0969, 10.3199],\n",
      "        ...,\n",
      "        [ 0.3527,  0.3115,  0.3528,  ..., -0.2930, -0.0969, -0.0969],\n",
      "        [-1.1689, -1.0646, -0.4599,  ..., -0.2930, -0.0969, -0.0969],\n",
      "        [-1.1689, -1.0646, -1.5436,  ...,  3.4128, -0.0969, -0.0969]])\n"
     ]
    }
   ],
   "source": [
    "features = np.array(feature)\n",
    "labels = np.array(label).reshape(-1,1)\n",
    "features = StandardScaler().fit_transform(features)\n",
    "features = torch.tensor(features, dtype = torch.float32)\n",
    "labels = torch.tensor(labels,dtype = torch.float32)\n",
    "print(features.shape, labels.shape)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([430, 1])\n",
      "tensor([[-0.2453],\n",
      "        [-0.2508],\n",
      "        [-0.2453],\n",
      "        [-0.2507],\n",
      "        [-0.2519],\n",
      "        [-0.2526],\n",
      "        [-0.2519],\n",
      "        [-0.2526],\n",
      "        [-0.2519],\n",
      "        [-0.2519],\n",
      "        [-0.2508],\n",
      "        [-0.2519],\n",
      "        [-0.2500],\n",
      "        [-0.2486],\n",
      "        [-0.2500],\n",
      "        [-0.2501],\n",
      "        [-0.2526],\n",
      "        [-0.2526],\n",
      "        [-0.2508],\n",
      "        [-0.2486],\n",
      "        [-0.2485],\n",
      "        [-0.2501],\n",
      "        [-0.2486],\n",
      "        [-0.2526],\n",
      "        [-0.2501],\n",
      "        [-0.2501],\n",
      "        [-0.2486],\n",
      "        [-0.2507],\n",
      "        [-0.2508],\n",
      "        [-0.2501],\n",
      "        [-0.2508],\n",
      "        [-0.2500],\n",
      "        [-0.2539],\n",
      "        [-0.2507],\n",
      "        [-0.2485],\n",
      "        [-0.2453],\n",
      "        [-0.2485],\n",
      "        [-0.2485],\n",
      "        [-0.2500],\n",
      "        [-0.2485],\n",
      "        [-0.2500],\n",
      "        [-0.2485],\n",
      "        [-0.2434],\n",
      "        [-0.2520],\n",
      "        [-0.2501],\n",
      "        [-0.2501],\n",
      "        [-0.2501],\n",
      "        [-0.2507],\n",
      "        [-0.2507],\n",
      "        [-0.2507],\n",
      "        [-0.2507],\n",
      "        [-0.2501],\n",
      "        [-0.2526],\n",
      "        [-0.2507],\n",
      "        [-0.2507],\n",
      "        [-0.2486],\n",
      "        [-0.2548],\n",
      "        [-0.2497],\n",
      "        [-0.2544],\n",
      "        [-0.2546],\n",
      "        [-0.2495],\n",
      "        [-0.2546],\n",
      "        [-0.2495],\n",
      "        [-0.2544],\n",
      "        [-0.2495],\n",
      "        [-0.2497],\n",
      "        [-0.2497],\n",
      "        [-0.2486],\n",
      "        [-0.2548],\n",
      "        [-0.2546],\n",
      "        [-0.2548],\n",
      "        [-0.2506],\n",
      "        [-0.2514],\n",
      "        [-0.2506],\n",
      "        [-0.2504],\n",
      "        [-0.2504],\n",
      "        [-0.2539],\n",
      "        [-0.2514],\n",
      "        [-0.2510],\n",
      "        [-0.2506],\n",
      "        [-0.2514],\n",
      "        [-0.2510],\n",
      "        [-0.2510],\n",
      "        [-0.2504],\n",
      "        [-0.2486],\n",
      "        [-0.2539],\n",
      "        [-0.2493],\n",
      "        [-0.2521],\n",
      "        [-0.2547],\n",
      "        [-0.2547],\n",
      "        [-0.2547],\n",
      "        [-0.2501],\n",
      "        [-0.2473],\n",
      "        [-0.2473],\n",
      "        [-0.2493],\n",
      "        [-0.2491],\n",
      "        [-0.2491],\n",
      "        [-0.2483],\n",
      "        [-0.2468],\n",
      "        [-0.2486],\n",
      "        [-0.2521],\n",
      "        [-0.2521],\n",
      "        [-0.2545],\n",
      "        [-0.2508],\n",
      "        [-0.2525],\n",
      "        [-0.2525],\n",
      "        [-0.2508],\n",
      "        [-0.2545],\n",
      "        [-0.2545],\n",
      "        [-0.2501],\n",
      "        [-0.2530],\n",
      "        [-0.2547],\n",
      "        [-0.2440],\n",
      "        [-0.2440],\n",
      "        [-0.2484],\n",
      "        [-0.2484],\n",
      "        [-0.2514],\n",
      "        [-0.2514],\n",
      "        [-0.2502],\n",
      "        [-0.2502],\n",
      "        [-0.2476],\n",
      "        [-0.2476],\n",
      "        [-0.2514],\n",
      "        [-0.2526],\n",
      "        [-0.2514],\n",
      "        [-0.2526],\n",
      "        [-0.2506],\n",
      "        [-0.2482],\n",
      "        [-0.2512],\n",
      "        [-0.2512],\n",
      "        [-0.2506],\n",
      "        [-0.2482],\n",
      "        [-0.2518],\n",
      "        [-0.2518],\n",
      "        [-0.2469],\n",
      "        [-0.2481],\n",
      "        [-0.2481],\n",
      "        [-0.2480],\n",
      "        [-0.2481],\n",
      "        [-0.2481],\n",
      "        [-0.2492],\n",
      "        [-0.2549],\n",
      "        [-0.2470],\n",
      "        [-0.2470],\n",
      "        [-0.2480],\n",
      "        [-0.2503],\n",
      "        [-0.2492],\n",
      "        [-0.2491],\n",
      "        [-0.2485],\n",
      "        [-0.2498],\n",
      "        [-0.2491],\n",
      "        [-0.2498],\n",
      "        [-0.2463],\n",
      "        [-0.2479],\n",
      "        [-0.2544],\n",
      "        [-0.2501],\n",
      "        [-0.2501],\n",
      "        [-0.2477],\n",
      "        [-0.2435],\n",
      "        [-0.2435],\n",
      "        [-0.2488],\n",
      "        [-0.2482],\n",
      "        [-0.2482],\n",
      "        [-0.2491],\n",
      "        [-0.2476],\n",
      "        [-0.2476],\n",
      "        [-0.2491],\n",
      "        [-0.2482],\n",
      "        [-0.2463],\n",
      "        [-0.2463],\n",
      "        [-0.2491],\n",
      "        [-0.2481],\n",
      "        [-0.2481],\n",
      "        [-0.2441],\n",
      "        [-0.2441],\n",
      "        [-0.2481],\n",
      "        [-0.2441],\n",
      "        [-0.2482],\n",
      "        [-0.2515],\n",
      "        [-0.2505],\n",
      "        [-0.2512],\n",
      "        [-0.2474],\n",
      "        [-0.2474],\n",
      "        [-0.2539],\n",
      "        [-0.2494],\n",
      "        [-0.2494],\n",
      "        [-0.2477],\n",
      "        [-0.2477],\n",
      "        [-0.2539],\n",
      "        [-0.2539],\n",
      "        [-0.2482],\n",
      "        [-0.2511],\n",
      "        [-0.2490],\n",
      "        [-0.2432],\n",
      "        [-0.2431],\n",
      "        [-0.2539],\n",
      "        [-0.2490],\n",
      "        [-0.2548],\n",
      "        [-0.2436],\n",
      "        [-0.2514],\n",
      "        [-0.2504],\n",
      "        [-0.2504],\n",
      "        [-0.2539],\n",
      "        [-0.2512],\n",
      "        [-0.2547],\n",
      "        [-0.2548],\n",
      "        [-0.2440],\n",
      "        [-0.2440],\n",
      "        [-0.2469],\n",
      "        [-0.2539],\n",
      "        [-0.2539],\n",
      "        [-0.2503],\n",
      "        [-0.2496],\n",
      "        [-0.2485],\n",
      "        [-0.2496],\n",
      "        [-0.2549],\n",
      "        [-0.2492],\n",
      "        [-0.2503],\n",
      "        [-0.2468],\n",
      "        [-0.2468],\n",
      "        [-0.2488],\n",
      "        [-0.2477],\n",
      "        [-0.2479],\n",
      "        [-0.2477],\n",
      "        [-0.2505],\n",
      "        [-0.2474],\n",
      "        [-0.2491],\n",
      "        [-0.2515],\n",
      "        [-0.2491],\n",
      "        [-0.2494],\n",
      "        [-0.2491],\n",
      "        [-0.2512],\n",
      "        [-0.2512],\n",
      "        [-0.2544],\n",
      "        [-0.2505],\n",
      "        [-0.2539],\n",
      "        [-0.2539],\n",
      "        [-0.2475],\n",
      "        [-0.2475],\n",
      "        [-0.2430],\n",
      "        [-0.2527],\n",
      "        [-0.2539],\n",
      "        [-0.2430],\n",
      "        [-0.2432],\n",
      "        [-0.2533],\n",
      "        [-0.2533],\n",
      "        [-0.2546],\n",
      "        [-0.2492],\n",
      "        [-0.2492],\n",
      "        [-0.2518],\n",
      "        [-0.2510],\n",
      "        [-0.2510],\n",
      "        [-0.2498],\n",
      "        [-0.2498],\n",
      "        [-0.2526],\n",
      "        [-0.2526],\n",
      "        [-0.2523],\n",
      "        [-0.2508],\n",
      "        [-0.2504],\n",
      "        [-0.2505],\n",
      "        [-0.2526],\n",
      "        [-0.2505],\n",
      "        [-0.2508],\n",
      "        [-0.2509],\n",
      "        [-0.2505],\n",
      "        [-0.2476],\n",
      "        [-0.2508],\n",
      "        [-0.2515],\n",
      "        [-0.2516],\n",
      "        [-0.2524],\n",
      "        [-0.2508],\n",
      "        [-0.2499],\n",
      "        [-0.2520],\n",
      "        [-0.2535],\n",
      "        [-0.2520],\n",
      "        [-0.2516],\n",
      "        [-0.2550],\n",
      "        [-0.2497],\n",
      "        [-0.2493],\n",
      "        [-0.2529],\n",
      "        [-0.2502],\n",
      "        [-0.2544],\n",
      "        [-0.2516],\n",
      "        [-0.2477],\n",
      "        [-0.2493],\n",
      "        [-0.2500],\n",
      "        [-0.2529],\n",
      "        [-0.2493],\n",
      "        [-0.2508],\n",
      "        [-0.2529],\n",
      "        [-0.2501],\n",
      "        [-0.2486],\n",
      "        [-0.2497],\n",
      "        [-0.2527],\n",
      "        [-0.2477],\n",
      "        [-0.2540],\n",
      "        [-0.2549],\n",
      "        [-0.2511],\n",
      "        [-0.2529],\n",
      "        [-0.2498],\n",
      "        [-0.2476],\n",
      "        [-0.2478],\n",
      "        [-0.2549],\n",
      "        [-0.1830],\n",
      "        [-0.2490],\n",
      "        [-0.2522],\n",
      "        [-0.2478],\n",
      "        [-0.2510],\n",
      "        [-0.2549],\n",
      "        [-0.1569],\n",
      "        [-0.1830],\n",
      "        [-0.2522],\n",
      "        [-0.2524],\n",
      "        [-0.2558],\n",
      "        [-0.2524],\n",
      "        [-0.2517],\n",
      "        [-0.2507],\n",
      "        [-0.2500],\n",
      "        [-0.2535],\n",
      "        [-0.2436],\n",
      "        [-0.2500],\n",
      "        [-0.2449],\n",
      "        [-0.2558],\n",
      "        [-0.2488],\n",
      "        [-0.2515],\n",
      "        [-0.2477],\n",
      "        [-0.2512],\n",
      "        [-0.2488],\n",
      "        [-0.0586],\n",
      "        [-0.2517],\n",
      "        [-0.2436],\n",
      "        [-0.2531],\n",
      "        [-0.2547],\n",
      "        [-0.2507],\n",
      "        [-0.2542],\n",
      "        [-0.2512],\n",
      "        [-0.2520],\n",
      "        [-0.2538],\n",
      "        [-0.2515],\n",
      "        [-0.2522],\n",
      "        [-0.2477],\n",
      "        [-0.2484],\n",
      "        [-0.2484],\n",
      "        [-0.2484],\n",
      "        [-0.2511],\n",
      "        [-0.2486],\n",
      "        [-0.2486],\n",
      "        [-0.2504],\n",
      "        [-0.2489],\n",
      "        [-0.2507],\n",
      "        [-0.2538],\n",
      "        [-0.2477],\n",
      "        [-0.2481],\n",
      "        [-0.2523],\n",
      "        [-0.2529],\n",
      "        [-0.2489],\n",
      "        [-0.2547],\n",
      "        [-0.2522],\n",
      "        [-0.2504],\n",
      "        [-0.2477],\n",
      "        [-0.2488],\n",
      "        [-0.2498],\n",
      "        [-0.2547],\n",
      "        [-0.2504],\n",
      "        [-0.2529],\n",
      "        [-0.2547],\n",
      "        [-0.2558],\n",
      "        [-0.2491],\n",
      "        [-0.2558],\n",
      "        [-0.2496],\n",
      "        [-0.2521],\n",
      "        [-0.2480],\n",
      "        [-0.2517],\n",
      "        [-0.2485],\n",
      "        [-0.2511],\n",
      "        [-0.2511],\n",
      "        [-0.2476],\n",
      "        [-0.2524],\n",
      "        [-0.2513],\n",
      "        [-0.2485],\n",
      "        [-0.2511],\n",
      "        [-0.2518],\n",
      "        [-0.2507],\n",
      "        [-0.2495],\n",
      "        [-0.2492],\n",
      "        [-0.2496],\n",
      "        [-0.2511],\n",
      "        [-0.2517],\n",
      "        [-0.2517],\n",
      "        [-0.2497],\n",
      "        [-0.2513],\n",
      "        [-0.2517],\n",
      "        [-0.2497],\n",
      "        [-0.2484],\n",
      "        [-0.2517],\n",
      "        [-0.2306],\n",
      "        [-0.2368],\n",
      "        [-0.2496],\n",
      "        [-0.2473],\n",
      "        [-0.2495],\n",
      "        [-0.2491],\n",
      "        [-0.2445],\n",
      "        [-0.2463],\n",
      "        [-0.2508],\n",
      "        [-0.2495],\n",
      "        [-0.2493],\n",
      "        [-0.2505],\n",
      "        [-0.2479],\n",
      "        [-0.2526],\n",
      "        [-0.2496],\n",
      "        [-0.2533],\n",
      "        [-0.2501],\n",
      "        [-0.2464],\n",
      "        [-0.2498],\n",
      "        [-0.2505],\n",
      "        [-0.2509],\n",
      "        [-0.2524],\n",
      "        [-0.2528],\n",
      "        [-0.2515],\n",
      "        [-0.2547],\n",
      "        [-0.2507],\n",
      "        [-0.2496],\n",
      "        [-0.2516],\n",
      "        [-0.2528],\n",
      "        [-0.2549],\n",
      "        [-0.2547],\n",
      "        [-0.2548],\n",
      "        [-0.2517],\n",
      "        [-0.2480],\n",
      "        [-0.2545]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(20,10),\n",
    "    torch.nn.LeakyReLU(), # activation functions\n",
    "    torch.nn.Linear(10,6),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(6,9),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(9,1)\n",
    ")\n",
    "\n",
    "preds = model(features)\n",
    "print(preds.shape)\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4302.6167, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse_loss(preds, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(55.1989, grad_fn=<MseLossBackward0>)\n",
      "tensor(13.8363, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7278, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.4039, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1639, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5631, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9717, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2947, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0836, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6920, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 15000\n",
    "for epoch in range(epochs):\n",
    "    preds = model(features)\n",
    "    loss = mse_loss(preds, labels)\n",
    "    \n",
    "    if (epoch+1)%(epochs//10) == 0:\n",
    "        print(loss)\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49.3180], grad_fn=<AddBackward0>) tensor([50.0400])\n"
     ]
    }
   ],
   "source": [
    "x = 100\n",
    "preds = model(features[x])\n",
    "print(preds,labels[x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dcc23356357f8276d44a785f9d0a2aa02edb775f35b369ed56d20e41d916bba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
